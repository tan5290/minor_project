# K NEAREST NEIGHBOUR

from sklearn.neighbors import NearestNeighbors
import numpy as np
X = np.array([[0, -1], [2, -1], [-9, -2], [0, 1], [2, 4], [6, 2]])
nb = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
distances, indices = nb.kneighbors(X)
indices
distances
nb.kneighbors_graph(X).toarray()
from sklearn.cluster import SpectralClustering
import numpy as np
X = np.array([[1, 1], [2, 1], [1, 0],[4, 7], [3, 5], [3, 6]])
clustering = SpectralClustering(n_clusters=2,assign_labels='discretize',random_state=0).fit(X)
clustering.labels_
clustering
SpectralClustering(assign_labels='discretize', n_clusters=2,random_state=0)
SpectralClustering(assign_labels='discretize', n_clusters=3,random_state=0)
clustering
import matplotlib.pyplot as plt
plt.scatter(X[:,0],X[:,1])

#DECISCION TREE

import pandas
from sklearn import tree
import pydotplus
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
df = pandas.read_csv("shows.csv")
d = {'UK': 0, 'USA': 1, 'N': 2}
df['Nationality'] = df['Nationality'].map(d)
d = {'YES': 1, 'NO': 0}
df['Go'] = df['Go'].map(d)
print(df)
features = ['Age', 'Experience', 'Rank', 'Nationality']
X = df[features]
y = df['Go']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state = 1)
dtree = DecisionTreeClassifier()
dtree = dtree.fit(X_train, y_train)
y_pred = dtree.predict(X_test)
y_pred
from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average='macro')
from sklearn import metrics 
confusion_matrix = metrics.confusion_matrix(y_test, y_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()

#RIDGE_LASSO_BAYSIAN

 from sklearn.datasets import make_regression
 from matplotlib import pyplot as plt
 import numpy as np
 from sklearn.linear_model import Ridge
 X, y, coefficients = make_regression(
 n_samples=50,
 n_features=1,
 n_informative=1,
 n_targets=1,
 noise=5,
 coef=True,
 random_state=1).
 n, m = X.shape
I = np.identity(m)
alpha = 1
w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X) + alpha * I), X.T), y)
w
plt.scatter(X, y)
plt.plot(X, w*X, c='red')
rr = Ridge(alpha=1)
rr.fit(X, y)
w = rr.coef_
w
plt.scatter(X, y)
plt.plot(X, w*X, c='red')
rr = Ridge(alpha=10)
rr.fit(X, y)
w = rr.coef_[0]
plt.scatter(X, y)
plt.plot(X, w*X, c='red')
rr = Ridge(alpha=100)
rr.fit(X, y)
w = rr.coef_[0]
plt.scatter(X, y)
plt.plot(X, w*X, c='red')
import numpy
#X represents the size of a tumor in centimeters.
X = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52,
3.69, 5.88]).reshape(-1,1)

#y represents whether or not the tumor is cancerous (0 for "No", 1 for"Yes").
y = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])
from sklearn import linear_model
logr = linear_model.LassoCV()
logr.fit(X,y)
#predict if tumor is cancerous where the size is 4.46mm:
predicted = logr.predict(numpy.array([4.46]).reshape(-1,1))
print(predicted)
from sklearn import linear_model
logr = linear_model.BayesianRidge()
logr.fit(X,y)
#predict if tumor is cancerous where the size is 3.46mm:
predicted = logr.predict(numpy.array([3.46]).reshape(-1,1))
print(predicted)
